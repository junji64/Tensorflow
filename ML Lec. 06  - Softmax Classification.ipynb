{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Multinomial classification\n",
    "\n",
    "For many instances ...\n",
    "$$ \\begin{bmatrix} \n",
    "w_{A1} & w_{A2} & w_{A3} \\\\\n",
    "w_{B1} & w_{B2} & w_{B3} \\\\\n",
    "w_{C1} & w_{C2} & w_{C3} \\\\\n",
    "\\end{bmatrix} \\cdot \n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3  \\end{bmatrix}\n",
    "= \n",
    " \\begin{bmatrix} \n",
    " w_{A1}x_1 + w_{A2}x_2 + w_{A3}x_3 \\\\\n",
    " w_{B1}x_1 + w_{B2}x_2 + w_{B3}x_3 \\\\\n",
    " w_{C1}x_1 + w_{C2}x_2 + w_{C3}x_3 \\\\\n",
    " \\end{bmatrix}\n",
    " =\n",
    " \\begin{bmatrix} Y_A \\\\ Y_B \\\\ Y_C  \n",
    " \\end{bmatrix}  \n",
    " $$\n",
    "  \n",
    "\n",
    "#### Softmax\n",
    "\n",
    "\n",
    "\n",
    "#### Cross-Entropy\n",
    "\n",
    "$$D(S,L) = -\\sum_i L_i \\cdot \\log(S_i) $$\n",
    "\n",
    "\n",
    "#### Logistic cost vs Cross-entropy\n",
    "\n",
    "$$C(H(x), y) = y \\log(H(x)) - (1-y)\\log (1 -H(x)) $$\n",
    "\n",
    "$$ D(S,L) = \\sum_i L_i \\log(S_i) $$\n",
    "\n",
    "#### Cost function\n",
    "\n",
    "$$\n",
    "Loss = {1 \\over N} \\sum_i D (S(WX_i+ b), L_i) \n",
    "$$\n",
    "\n",
    "#### Gradient Descent\n",
    "$- \\alpha \\Delta L(w_1, w_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 6-1\n",
    "\n",
    "#### Softmax function\n",
    "\n",
    "Logistic classifier's Scores (e.g. [2., 1., 0.1]) ===> Probabilities (e.g. [0.7, 0.2, 0.1])\n",
    "\n",
    "$$ XW = y$$\n",
    "$$ S(y_i) = { e^{y_i} \\over \\sum_j e^{y_j}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5], [1,2,5,6], [1,6,6,6], [1,7,7,7]]\n",
    "y_data = [[0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0], [1,0,0], [1,0,0]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "#\n",
    "#\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "#\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
