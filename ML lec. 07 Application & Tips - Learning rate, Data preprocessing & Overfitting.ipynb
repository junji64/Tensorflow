{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "\n",
    "$$ W := W - \\alpha { \\partial \\over \\partial W} \\text{cost}(W) $$\n",
    "Where $\\alpha$ is **the learning rate**.\n",
    "\n",
    "Large learning rate ===> overshooting\n",
    "\n",
    "Small learning rate ===> takes too long, stops at local minimum\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/0*QwE8M4MupSdqA3M4.png\" width=\"400\">\n",
    "\n",
    "Try several learning rates\n",
    "* Observe the cost function\n",
    "* Check it goes down in a reasonable rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing (for gradient descent)\n",
    "\n",
    "\n",
    "$$ \\hat{x}_j = { x_j - \\mu_j \\over \\sigma_j} $$\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/nn2/prepro1.jpeg\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_normal[:,0] = ( X[:,0] - X[:,0].mean()) / X[:,0].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "* training data 에 과도하게 최적화 되는 현상\n",
    "* real data 에 대해선 잘 동작되지 않는다.\n",
    "\n",
    "<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2015/02/underfitting-overfitting.png\" width=\"450\">\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*JZbxrdzabrT33Yl-LrmShw.png\" width=\"450\">\n",
    "\n",
    "Solutions for overfitting\n",
    "\n",
    "* More training data\n",
    "* features 의 개수를 줄인다\n",
    "* Regularization\n",
    "\n",
    "#### Regularization\n",
    "\n",
    "Let's not have too big numbers in the weight\n",
    "\n",
    "$$ Loss = { 1 \\over N} \\sum_i D(S(WX_i + b), L_i) + \\lambda \\sum W^2 $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning & test data sets\n",
    "\n",
    "### Traing, Validation and test sets\n",
    "\n",
    "1. training data 에 대해서는 이미 적응된 상태이기 때문에, 실제로 잘 동작하는지 확인을 할 수 없다. ==> test data 가 필요함!\n",
    "2. 학습을 제대로 하기 위해서는 적절한 learning rate와 regularization 세기를 찾기위한 validation 작업이 있어야 한다. ==> Validation data 가 필요함!\n",
    "\n",
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/9951E5445AAE1BE025\" width=\"400\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online learning\n",
    "\n",
    "* 너무 많은 양의 데이터가 있을때, 분할하여 나누어 학습을 시행\n",
    "* Data 가 지속적으로 유입되는 경우 사용되기도 한다.\n",
    "\n",
    "\n",
    "#### MINIST Dataset\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*9Mjoc_J0JR294YwHGXwCeg.jpeg\" width=\"350\">\n",
    "* train-images-idx3-ubyte.gz: training set images (9912422 bytes)\n",
    "* train-labels-idx1-ubyte.gz: training set labels (28881 bytes)\n",
    "* t10k-images-idx3-ubyte.gz: test set images (1648877 bytes)\n",
    "* t10k-labels-idx1-ubyte.gz: test set labels (4542 bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "* How many of your predictions are correct?\n",
    "* 95% ~ 99% ?\n",
    "* Check out the lab video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 7-1 : Learning rate & Evaluation\n",
    "\n",
    "### Training and Test datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.8400161 [[-0.5092531  -0.55309695  0.15962125]\n",
      " [ 0.24778174 -0.35704064 -0.8072263 ]\n",
      " [-0.12941405  0.8625646   0.43475065]]\n",
      "Prediction:  [0 1 1]\n",
      "Accuracy:  0.0\n",
      "50 0.9097175 [[-1.1111655  -0.6879001   0.89633685]\n",
      " [-0.14410932 -0.4410277  -0.33134824]\n",
      " [ 0.3380303   0.6408248   0.18904623]]\n",
      "Prediction:  [2 2 2]\n",
      "Accuracy:  1.0\n",
      "100 0.74956024 [[-1.4903545  -0.75329983  1.3409252 ]\n",
      " [-0.18470137 -0.4083957  -0.3233881 ]\n",
      " [ 0.5356226   0.6467464  -0.01446732]]\n",
      "Prediction:  [2 2 2]\n",
      "Accuracy:  1.0\n",
      "150 0.6704539 [[-1.7779881  -0.7806483   1.6559069 ]\n",
      " [-0.17617482 -0.3839322  -0.35637805]\n",
      " [ 0.6428734   0.6450428  -0.12001431]]\n",
      "Prediction:  [2 2 2]\n",
      "Accuracy:  1.0\n",
      "200 0.62001187 [[-2.0178754  -0.7861436   1.9012904 ]\n",
      " [-0.16487505 -0.3646542  -0.38695562]\n",
      " [ 0.72674996  0.6383557  -0.19720381]]\n",
      "Prediction:  [2 2 2]\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[1,2,1], [1,3,2], [1,3,4], [1,5,5], [1,7,5], [1,2,5], [1,6,6], [1,7,7]]\n",
    "y_data = [[0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0], [1,0,0], [1,0,0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2,1,1], [3,1,2], [3,3,4]]\n",
    "y_test = [[0,0,1], [0,0,1], [0,0,1]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "W = tf.Variable(tf.random_normal([3,3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer],\n",
    "                                      feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 50 == 0:\n",
    "            print(step, cost_val, W_val)\n",
    "        \n",
    "            # prediction\n",
    "            print(\"Prediction: \", sess.run(prediction, feed_dict={X: x_test}))\n",
    "            print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.2973762 [[-0.11171157 -0.1398418  -0.12182035]\n",
      " [ 0.39755306 -1.730458   -1.9189985 ]\n",
      " [-0.76861125 -0.10465971  0.4708441 ]]\n",
      "Prediction:  [2 2 0]\n",
      "Accuracy:  0.6666667\n",
      "50 3.2973762 [[-0.11171157 -0.1398418  -0.12182035]\n",
      " [ 0.39755306 -1.730458   -1.9189985 ]\n",
      " [-0.76861125 -0.10465971  0.4708441 ]]\n",
      "Prediction:  [2 2 0]\n",
      "Accuracy:  0.6666667\n",
      "100 3.2973762 [[-0.11171157 -0.1398418  -0.12182035]\n",
      " [ 0.39755306 -1.730458   -1.9189985 ]\n",
      " [-0.76861125 -0.10465971  0.4708441 ]]\n",
      "Prediction:  [2 2 0]\n",
      "Accuracy:  0.6666667\n",
      "150 3.2973762 [[-0.11171157 -0.1398418  -0.12182035]\n",
      " [ 0.39755306 -1.730458   -1.9189985 ]\n",
      " [-0.76861125 -0.10465971  0.4708441 ]]\n",
      "Prediction:  [2 2 0]\n",
      "Accuracy:  0.6666667\n",
      "200 3.2973762 [[-0.11171157 -0.1398418  -0.12182035]\n",
      " [ 0.39755306 -1.730458   -1.9189985 ]\n",
      " [-0.76861125 -0.10465971  0.4708441 ]]\n",
      "Prediction:  [2 2 0]\n",
      "Accuracy:  0.6666667\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-10).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer],\n",
    "                                      feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 50 == 0:\n",
    "            print(step, cost_val, W_val)\n",
    "        \n",
    "            # prediction\n",
    "            print(\"Prediction: \", sess.run(prediction, feed_dict={X: x_test}))\n",
    "            print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-normalized inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  4440626000000.0 \n",
      "Prediction:  [[1487265.9]\n",
      " [2993255.5]\n",
      " [2354841.8]\n",
      " [1650949.5]\n",
      " [1945603.2]\n",
      " [1961970.9]\n",
      " [1798269.9]\n",
      " [2289351.5]]\n",
      "200 Cost:  nan \n",
      "Prediction:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "400 Cost:  nan \n",
      "Prediction:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "600 Cost:  nan \n",
      "Prediction:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "800 Cost:  nan \n",
      "Prediction:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "1000 Cost:  nan \n",
      "Prediction:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "1200 Cost:  nan \n",
      "Prediction:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "1400 Cost:  nan \n",
      "Prediction:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "1600 Cost:  nan \n",
      "Prediction:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "1800 Cost:  nan \n",
      "Prediction:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "2000 Cost:  nan \n",
      "Prediction:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "              [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "              [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "              [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "              [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "              [819, 823, 1198100, 816, 820.450012],\n",
    "              [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "              [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:,[-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([4,1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                                  feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 200 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction: \", hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.         0.         1.         1.        ]\n",
      " [0.70548491 0.70439552 1.         0.71881783 0.83755792]\n",
      " [0.54412549 0.50274824 0.57608696 0.60646801 0.6606331 ]\n",
      " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
      " [0.51436    0.4258239  0.30434783 0.58504805 0.42624401]\n",
      " [0.49556179 0.4258239  0.31521739 0.48131134 0.49276137]\n",
      " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
      " [0.         0.07747099 0.5326087  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "xy = preprocessing.MinMaxScaler().fit_transform(xy)\n",
    "print(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  0.045821086 \n",
      "Prediction:  [[1.2555099 ]\n",
      " [0.4145739 ]\n",
      " [0.50768864]\n",
      " [0.6131461 ]\n",
      " [0.6193191 ]\n",
      " [0.5854387 ]\n",
      " [0.31217027]\n",
      " [0.08068472]]\n",
      "200 Cost:  0.045788623 \n",
      "Prediction:  [[1.2551979 ]\n",
      " [0.41446897]\n",
      " [0.5075335 ]\n",
      " [0.61293745]\n",
      " [0.6191236 ]\n",
      " [0.5852525 ]\n",
      " [0.31201276]\n",
      " [0.08059844]]\n",
      "400 Cost:  0.04575628 \n",
      "Prediction:  [[1.2548879 ]\n",
      " [0.4143655 ]\n",
      " [0.50737953]\n",
      " [0.61272943]\n",
      " [0.61892927]\n",
      " [0.5850673 ]\n",
      " [0.3118555 ]\n",
      " [0.08051217]]\n",
      "600 Cost:  0.045724012 \n",
      "Prediction:  [[1.2545781 ]\n",
      " [0.4142621 ]\n",
      " [0.50722563]\n",
      " [0.6125215 ]\n",
      " [0.61873496]\n",
      " [0.58488214]\n",
      " [0.31169826]\n",
      " [0.08042589]]\n",
      "800 Cost:  0.045691893 \n",
      "Prediction:  [[1.2542711 ]\n",
      " [0.4141608 ]\n",
      " [0.5070735 ]\n",
      " [0.6123149 ]\n",
      " [0.6185424 ]\n",
      " [0.5846984 ]\n",
      " [0.31154162]\n",
      " [0.08033961]]\n",
      "1000 Cost:  0.045659848 \n",
      "Prediction:  [[1.2539642 ]\n",
      " [0.41405943]\n",
      " [0.5069213 ]\n",
      " [0.61210823]\n",
      " [0.6183498 ]\n",
      " [0.5845146 ]\n",
      " [0.311385  ]\n",
      " [0.08025336]]\n",
      "1200 Cost:  0.045628134 \n",
      "Prediction:  [[1.2536604 ]\n",
      " [0.41396135]\n",
      " [0.5067724 ]\n",
      " [0.61190486]\n",
      " [0.61816037]\n",
      " [0.584334  ]\n",
      " [0.3112316 ]\n",
      " [0.08017027]]\n",
      "1400 Cost:  0.04559673 \n",
      "Prediction:  [[1.2533594 ]\n",
      " [0.41386592]\n",
      " [0.5066262 ]\n",
      " [0.61170423]\n",
      " [0.6179737 ]\n",
      " [0.5841562 ]\n",
      " [0.31108096]\n",
      " [0.08008999]]\n",
      "1600 Cost:  0.04556538 \n",
      "Prediction:  [[1.2530584 ]\n",
      " [0.4137706 ]\n",
      " [0.50648   ]\n",
      " [0.6115035 ]\n",
      " [0.61778706]\n",
      " [0.5839784 ]\n",
      " [0.3109303 ]\n",
      " [0.08000967]]\n",
      "1800 Cost:  0.045534108 \n",
      "Prediction:  [[1.2527579 ]\n",
      " [0.4136756 ]\n",
      " [0.50633407]\n",
      " [0.61130303]\n",
      " [0.6176007 ]\n",
      " [0.58380085]\n",
      " [0.31077972]\n",
      " [0.07992935]]\n",
      "2000 Cost:  0.045502964 \n",
      "Prediction:  [[1.2524599 ]\n",
      " [0.41358227]\n",
      " [0.50618947]\n",
      " [0.6111034 ]\n",
      " [0.61741555]\n",
      " [0.58362454]\n",
      " [0.3106294 ]\n",
      " [0.07984903]]\n"
     ]
    }
   ],
   "source": [
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:,[-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([4,1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                                  feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 200 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction: \", hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 7-2 : MNIST Dataset\n",
    "\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*9Mjoc_J0JR294YwHGXwCeg.jpeg\" width=\"350\">\n",
    "* train-images-idx3-ubyte.gz: training set images (9912422 bytes)\n",
    "* train-labels-idx1-ubyte.gz: training set labels (28881 bytes)\n",
    "* t10k-images-idx3-ubyte.gz: test set images (1648877 bytes)\n",
    "* t10k-labels-idx1-ubyte.gz: test set labels (4542 bytes)\n",
    "\n",
    "<img src=\"MNIST-1.png\" width=\"450\">\n",
    "<img src=\"MNIST-2.png\" width=\"450\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch:  0001 cost =  2.573043787\n",
      "Epoch:  0002 cost =  1.108057230\n",
      "Epoch:  0003 cost =  0.891606407\n",
      "Epoch:  0004 cost =  0.782689544\n",
      "Epoch:  0005 cost =  0.713577160\n",
      "Epoch:  0006 cost =  0.664424415\n",
      "Epoch:  0007 cost =  0.626811342\n",
      "Epoch:  0008 cost =  0.597096496\n",
      "Epoch:  0009 cost =  0.571814834\n",
      "Epoch:  0010 cost =  0.551115787\n",
      "Epoch:  0011 cost =  0.532834988\n",
      "Epoch:  0012 cost =  0.517178554\n",
      "Epoch:  0013 cost =  0.503191626\n",
      "Epoch:  0014 cost =  0.490593097\n",
      "Epoch:  0015 cost =  0.479100942\n",
      "Accuracy:  0.8878\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        print('Epoch: ', '%04d' %(epoch + 1), 'cost = ', '{:.9f}'.format(avg_cost))\n",
    "        \n",
    "    print(\"Accuracy: \", sess.run(accuracy,feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "Labels:  [8]\n",
      "Prediction: [4]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "r = random.randint(0, mnist.test.num_examples -1)\n",
    "print(mnist.test.labels[r:r+1])\n",
    "print(\"Labels: \", sess.run(tf.argmax(mnist.test.labels[r:r+1],1)))\n",
    "print(\"Prediction:\", sess.run(tf.argmax(hypothesis, 1),\n",
    "                             feed_dict={X: mnist.test.images[r:r+1]}))\n",
    "plt.imshow(mnist.test.images[r:r+1].reshape(28,28),\n",
    "          cmap = \"Greys\", interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
